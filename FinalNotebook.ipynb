{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook: Yolo Detection of Bacteria on Agar\n",
    "#### Dietrich Nigh\n",
    "\n",
    "## Business Understanding of the Problem\n",
    "\n",
    "Since the discovery of bacteria in the 17th century, scientists have been trying to identify and classify those tiny specs under the microscope. In the 19th century, Julius Richard Petri, a German physician working under the famous Robert Koch, developed his namesake, the Petri dish, for this purpose. He needed to reliably grow bacteria without risk of containmination so he could accurately study his specimens. Since that time, the classification of bacteria has come a long way. Many bacterial samples can be sequenced to elucidate all of their secrets. That said, the Petri dish is still a critical tool in the culturing and classification of bacterial samples. \n",
    "\n",
    "Presently, the identification of bacteria is a laborious and time consuming task. This is not to mention the years of training needed to properly perform the task. Even still mistakes can be made. To reduce the cost (both in time and money), recent years have seen an explosion of research into the construction machine learning models to correctly identify bacteria from a sample. Agar plates (Petri dishes with agar media) are a widely available, affordable, and effective means of growing isolated samples. If a model could be made to quickly and accurately differiante bacteria based on their growth, medical diagnostics could be done quickly with less training for technicians. For research applications, time spent memorizing such works as Bergey's Manual could be diverted elsewhere.\n",
    "\n",
    "The model chosen for this task is YOLO. YOLO is a single stage object detection model from Ultralytics with many hidden layers. YOLO is a single stage detector, meaning it performs regression around the object of interest and the classification of said image in parallel. This makes it much faster than dual-stage detectors, like Faster RCNN, which perform these tasks sequentially. The model is also relatively lightweight once trained. The model was developed for such tasks as live object detection after all. With this model we were able to construct a highly precise model.\n",
    "\n",
    "As this model is only good for 5 different species, I am hoping to gain the funding necessary to improve this model model. I am seeking this funding from NIH.\n",
    "\n",
    "<img src='https://editor.analyticsvidhya.com/uploads/1512812.png' width=\"540\" height=\"270\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary libraries\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "A test json file was read in. Keys and items were explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {    \"background\": \"bright\",\n",
    "    \"classes\": [\n",
    "        \"P.aeruginosa\"\n",
    "    ],\n",
    "    \"colonies_number\": 4,\n",
    "    \"labels\": [\n",
    "        {\n",
    "            \"class\": \"P.aeruginosa\",\n",
    "            \"height\": 307,\n",
    "            \"id\": 1,\n",
    "            \"width\": 307,\n",
    "            \"x\": 549,\n",
    "            \"y\": 1141\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"P.aeruginosa\",\n",
    "            \"height\": 190,\n",
    "            \"id\": 2,\n",
    "            \"width\": 190,\n",
    "            \"x\": 1667,\n",
    "            \"y\": 1880\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"P.aeruginosa\",\n",
    "            \"height\": 279,\n",
    "            \"id\": 3,\n",
    "            \"width\": 279,\n",
    "            \"x\": 510,\n",
    "            \"y\": 2300\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"P.aeruginosa\",\n",
    "            \"height\": 157,\n",
    "            \"id\": 4,\n",
    "            \"width\": 157,\n",
    "            \"x\": 3086,\n",
    "            \"y\": 2480\n",
    "        }\n",
    "    ],\n",
    "    \"sample_id\": 356\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['background', 'classes', 'colonies_number', 'labels', 'sample_id'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('class', 'P.aeruginosa'), ('height', 307), ('id', 1), ('width', 307), ('x', 549), ('y', 1141)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels'][0].items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Reformatting\n",
    "\n",
    "As I am using a YOLO model, data structure is quite important. The data was converted into kitti format and written into txt files. Then the data was reorganized into the proper repos, before repeating this process to convert the data to YOLO format. The kitti2yolo formatter belongs to ZeroEyes, Inc. I have gained permission before using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for number in range(0,18001): # create list of numbers range 0 - 18000\n",
    "    filenames.append(number)\n",
    "filenames.pop(0) # remove number 0 from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileswithoutcolonies = [] # place holder for files in \n",
    "for file in filenames:\n",
    "    kitlines = [] #converted lines to be placed in a txt file\n",
    " \n",
    "    with open(f'data/AGAR_dataset/dataset/{file}.json') as f:\n",
    "            data = json.load(f) # open json file\n",
    "            for colony in data['labels']: \n",
    "               species = colony['class'] # extract species\n",
    "               x =  colony['x'] # extract bounding box corner\n",
    "               y = colony['y'] # extract bounding box corner\n",
    "               x2 = colony['width'] + colony['x'] # calculate position of second corner\n",
    "               y2 = colony['height'] + colony['y'] # calculate position of second corner\n",
    "               kitlines.append(f'{species} 0.0 0 0.0 {x} {y} {x2} {y2} 0.00 0.00 0.00 0.00 0.00 0.00 0.00') # line to write to file\n",
    "               kitlines.append('\\n') # create a new line after file\n",
    "               f.close() # close the file\n",
    "\n",
    "\n",
    "    if len(kitlines) >= 1: # check if any colonies available\n",
    "        with open(f'./data/txtfiles/{file}.txt', 'w') as textfile: # create new file, file.txt with kitlines\n",
    "            for line in kitlines:\n",
    "                textfile.writelines(line)\n",
    "    else:\n",
    "        print(f'File {file} had no colonies labelled') # report if no files found and append to list\n",
    "        fileswithoutcolonies.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5728"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fileswithoutcolonies) # check number without colonies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24544"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(18000 - len(fileswithoutcolonies)) * 2 # check number with colonies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filenames: # put jpg files in same location as txt files\n",
    "    if file not in fileswithoutcolonies:\n",
    "        shutil.copyfile(src=f'./data/AGAR_dataset/dataset/{file}.jpg',dst=f'./data/txtfiles/{file}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was provided by ZeroEyes, Inc.\n",
    "\n",
    "def kitti2bboxes(kitti_file):\n",
    "    \"\"\"\n",
    "    Description: Convert kitti label file to arrays of bboxes and labels\n",
    "    :param kitti_file: Path to kitti label file\n",
    "    :return bboxes: 2D array of bboxes where each sub-array is bbox coordinates in the form of\n",
    "    [x_min, y_min, x_max, y_max]\n",
    "    :return labels: Array of labels corresponding to the bboxes array\n",
    "    \"\"\"\n",
    "\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    for line in open(kitti_file).readlines():\n",
    "        line_contents = line.split(' ')\n",
    "        assert len(line_contents) > 1, f'File {kitti_file} does not have required number of fields'\n",
    "        labels.append(line_contents[0])\n",
    "        bbox = [int(float(line_contents[4])), int(float(line_contents[5])),\n",
    "                int(float(line_contents[6])), int(float(line_contents[7]))]\n",
    "        bboxes.append(bbox)\n",
    "\n",
    "    return bboxes, labels\n",
    "def kitti2yolo(dataset_path, resolution=(960, 540), use_images=True,\n",
    "               class_map={'r_1': 'r_1', 'p_1': 'p_1'}):\n",
    "    \"\"\"\n",
    "    Create annotations for YOLO implementation in format:\n",
    "    class_id center_x center_y bbox_width bb_height\n",
    "    where all values are normalized and one label file is generated per image file\n",
    "    NOTE: This script will also generate a class dict that will be placed in the dab location\n",
    "    :param dataset_path: Path to dataset where labels will be converted. expects <dataset_path>/train/labels\n",
    "    :param resolution: Resolution to normalize over if use_images is False\n",
    "    :param use_images: Flag to use DAB image resolutions to normalize, will slow down\n",
    "    process\n",
    "    :param class_map: Class map to map labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Set paths and subdirectories\n",
    "    subsets = ['train', 'val', 'test']\n",
    "    assert os.path.exists(dataset_path), f'{dataset_path} not present'\n",
    "\n",
    "    # Create label ID map\n",
    "    current_label_value = 0\n",
    "    id_map = {}\n",
    "    for label_value in class_map.values():\n",
    "        if label_value not in id_map.keys():\n",
    "            id_map[label_value] = current_label_value\n",
    "            current_label_value += 1\n",
    "\n",
    "    # Iterate through subdirectory and create conversions. Converted files will be placed in yolo_v6_labels directory\n",
    "    for subdirectory in subsets:\n",
    "        kitti_labels_path = os.path.join(dataset_path, subdirectory, 'labels')\n",
    "        images_path = os.path.join(dataset_path, subdirectory, 'images')\n",
    "        yolo_v5_labels_path = os.path.join(dataset_path, subdirectory, 'yolo_v5_labels')\n",
    "\n",
    "        # Check that labels are present and that no yolo_v6 labels have been made yet\n",
    "        if not os.path.exists(kitti_labels_path):\n",
    "            print(f'Path not found for subset {subdirectory}, skipping')\n",
    "            continue\n",
    "\n",
    "        if not len(os.listdir(kitti_labels_path)) > 0:\n",
    "            print(f'No labels found in {kitti_labels_path}, skipping subset {subdirectory}')\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(yolo_v5_labels_path):\n",
    "            print(f'Labels present in {yolo_v5_labels_path}, these will be overwritten')\n",
    "            shutil.rmtree(yolo_v5_labels_path)\n",
    "        os.mkdir(yolo_v5_labels_path)\n",
    "\n",
    "        # Iterate through labels and create label format for YOLOv6\n",
    "        total_labels = len(os.listdir(kitti_labels_path))\n",
    "        for label_file in tqdm(os.listdir(kitti_labels_path), total=total_labels):\n",
    "\n",
    "            # Get image file to get image width and height\n",
    "            if use_images:\n",
    "                image_file = label_file.split('.txt')[0] + '.jpg'\n",
    "                image_file_path = os.path.join(images_path, image_file)\n",
    "                img = cv2.imread(image_file_path)\n",
    "                image_height = img.shape[0]\n",
    "                image_width = img.shape[1]\n",
    "            else:\n",
    "                image_height = resolution[1]\n",
    "                image_width = resolution[0]\n",
    "\n",
    "            # Create normalized labels\n",
    "            bboxes, labels = kitti2bboxes(os.path.join(kitti_labels_path, label_file))\n",
    "            yolo_v5_label_lines = []\n",
    "            for index in range(len(labels)):\n",
    "                if labels[index] not in class_map:\n",
    "                    continue\n",
    "                x_min = bboxes[index][0]\n",
    "                y_min = bboxes[index][1]\n",
    "                x_max = bboxes[index][2]\n",
    "                y_max = bboxes[index][3]\n",
    "                height = (y_max - y_min) / image_height\n",
    "                width = (x_max - x_min) / image_width\n",
    "                center_y = (y_min + ((y_max - y_min) / 2)) / image_height\n",
    "                center_x = (x_min + ((x_max - x_min) / 2)) / image_width\n",
    "                label = [labels[index]][0]\n",
    "                class_label = class_map[label]\n",
    "                class_id = id_map[class_label]\n",
    "                annotation_line = f'{class_id} {center_x} {center_y} {width} {height}'\n",
    "                yolo_v5_label_lines.append(annotation_line)\n",
    "\n",
    "            # Write YOLO v6 line\n",
    "            yolo_v5_file = os.path.join(yolo_v5_labels_path, label_file)\n",
    "            with open(yolo_v5_file, 'w+') as f:\n",
    "                for line in yolo_v5_label_lines:\n",
    "                    f.writelines(line + '\\n')\n",
    "\n",
    "    # Create label dict JSON\n",
    "    label_json_file = os.path.join(dataset_path, 'yolo_v5_id_map.json')\n",
    "    with open(label_json_file, 'w+') as f:\n",
    "        json.dump(id_map, f, indent=4)\n",
    "\n",
    "    print(f'Label values mapped to {id_map}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12272"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolating files with colonies in them\n",
    "good_file_nums = []\n",
    "for file in filenames:\n",
    "    if str(file) not in fileswithoutcolonies: # exclude files without colonies\n",
    "        good_file_nums.append(str(file)) # write numbers with colonies to good_file_nums\n",
    "\n",
    "len(good_file_nums) # check number of good images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nums = good_file_nums.copy() # copy for y in train-test\n",
    "\n",
    "train_val_nums, test_nums, train_val_y, test_y = train_test_split(good_file_nums, y_nums) # Train, test split\n",
    "train_nums, val_nums, train_y, val_y = train_test_split(train_val_nums, train_val_y) # Train, validation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6903 2301 3068\n"
     ]
    }
   ],
   "source": [
    "print(len(train_nums), len(val_nums), len(test_nums)) # confirming length of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for classes, needed by kitti2yolo\n",
    "\n",
    "label_dict = {'S.aureus': 'S.aureus', 'B.subtilis': 'B.subtilis',\n",
    "              'P.aeruginosa': 'P.aeruginosa', 'E.coli': 'E.coli', 'C.albicans': 'C.albicans'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move train data to new repo\n",
    "for file in train_nums:\n",
    "    shutil.copyfile(src=f'/home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/data/{file}.jpg',dst=f'./data/bacteria_data/train/images/{file}.jpg')\n",
    "    shutil.copyfile(src=f'/home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/data/{file}.txt',dst=f'./data/bacteria_data/train/labels/{file}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move test data to new repo\n",
    "for file in test_nums:\n",
    "    shutil.copyfile(src=f'/home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/data/{file}.jpg',\n",
    "                    dst=f'./data/bacteria_data/test/images/{file}.jpg')\n",
    "    shutil.copyfile(src=f'/home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/data/{file}.txt',\n",
    "                    dst=f'./data/bacteria_data/test/labels/{file}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move validation data to new repo\n",
    "for file in val_nums:\n",
    "    shutil.copyfile(src=f'/home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/data/{file}.jpg',\n",
    "                    dst=f'./data/bacteria_data/val/images/{file}.jpg')\n",
    "    shutil.copyfile(src=f'/home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/data/{file}.txt',\n",
    "                    dst=f'./data/bacteria_data/val/labels/{file}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6903/6903 [08:38<00:00, 13.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2301/2301 [02:48<00:00, 13.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3068/3068 [03:44<00:00, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label values mapped to {'S.aureus': 0, 'B.subtilis': 1, 'P.aeruginosa': 2, 'E.coli': 3, 'C.albicans': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# perform conversion from kitti data to yolo data\n",
    "kitti2yolo(dataset_path='./data/bacteria_data/', resolution=(1000, 1000), use_images=True, class_map= label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model constructed and trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ no model scale passed. Assuming scale='n'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]          \n",
      "YOLOv8 summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate fist model\n",
    "model = YOLO('./yolov8.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.69 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.67 ðŸš€ Python-3.8.10 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24252MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=./yolov8.yaml, data=./data.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
      "WARNING âš ï¸ no model scale passed. Assuming scale='n'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.Detect                [5, [64, 128, 256]]           \n",
      "YOLOv8 summary: 225 layers, 3011823 parameters, 3011807 gradients, 8.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/labels/train... 6903 images, 0 backgrounds, 21 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6903/6903 [00:03<00:00, 1983.85it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/11988.jpg: ignoring corrupt image/label: negative label values [   -0.47082]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/12160.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/12607.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0059      1.0006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/15250.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1492]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/3417.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.027]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/4302.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0051      1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/4327.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/4734.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0043]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/5034.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2452]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/5794.jpg: ignoring corrupt image/label: negative label values [   -0.16099]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/5934.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0122]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/6360.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0115]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/7119.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0007]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/7163.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1549]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/7354.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0259]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/7612.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.003]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/8100.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0124      1.0095]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/train/8658.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/labels/val... 2301 images, 1 backgrounds, 9 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2301/2301 [00:01<00:00, 2006.92it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/val/12269.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/val/12377.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0195]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/val/13672.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/val/3440.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0128]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/val/6750.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0068]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/val/8421.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0114]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/labels/val.cache\n",
      "Plotting labels to runs/detect/train4/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3      7.28G      3.396      3.181      2.133        173        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 431/431 [04:42<00:00,  1.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:27<00:00,  2.65it/s]\n",
      "                   all       2297      63585      0.304       0.34      0.237      0.112\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3      6.43G      1.797      1.678      1.233        223        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 431/431 [04:38<00:00,  1.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:25<00:00,  2.87it/s]\n",
      "                   all       2297      63585        0.5      0.532      0.478      0.246\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3      5.38G      1.577      1.395      1.116        376        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 431/431 [04:50<00:00,  1.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:32<00:00,  2.20it/s]\n",
      "                   all       2297      63585      0.556      0.584      0.552      0.306\n",
      "\n",
      "3 epochs completed in 0.261 hours.\n",
      "Optimizer stripped from runs/detect/train4/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train4/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train4/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.67 ðŸš€ Python-3.8.10 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24252MiB)\n",
      "YOLOv8 summary (fused): 168 layers, 3006623 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:31<00:00,  2.27it/s]\n",
      "                   all       2297      63585      0.557      0.583      0.552      0.306\n",
      "              S.aureus       2297      16485      0.686      0.643      0.655      0.329\n",
      "            B.subtilis       2297       5525      0.546      0.517      0.523        0.3\n",
      "          P.aeruginosa       2297      13773      0.612      0.488      0.533      0.293\n",
      "                E.coli       2297      16520       0.45       0.87      0.723      0.466\n",
      "            C.albicans       2297      11282      0.492      0.399      0.325       0.14\n",
      "Speed: 0.1ms preprocess, 0.6ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#train first model for 3 epochs\n",
    "model.train(data = './data.yaml', epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/test/13512.jpg: 640x640 5 S.aureuss, 21 E.colis, 3.4ms\n",
      "Speed: 9.5ms preprocess, 3.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.yolo.engine.results.Results object with attributes:\n",
       " \n",
       " _keys: ('boxes', 'masks', 'probs', 'keypoints')\n",
       " boxes: ultralytics.yolo.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'S.aureus', 1: 'B.subtilis', 2: 'P.aeruginosa', 3: 'E.coli', 4: 'C.albicans'}\n",
       " orig_img: array([[[ 69, 108, 110],\n",
       "         [ 67, 106, 108],\n",
       "         [ 64, 103, 105],\n",
       "         ...,\n",
       "         [ 99, 125, 139],\n",
       "         [ 96, 122, 136],\n",
       "         [ 96, 122, 136]],\n",
       " \n",
       "        [[ 67, 106, 108],\n",
       "         [ 68, 107, 109],\n",
       "         [ 68, 107, 109],\n",
       "         ...,\n",
       "         [103, 129, 143],\n",
       "         [103, 129, 143],\n",
       "         [104, 130, 144]],\n",
       " \n",
       "        [[ 67, 106, 108],\n",
       "         [ 65, 104, 106],\n",
       "         [ 62, 101, 103],\n",
       "         ...,\n",
       "         [103, 129, 141],\n",
       "         [105, 131, 143],\n",
       "         [107, 133, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[121, 184, 198],\n",
       "         [153, 216, 230],\n",
       "         [114, 177, 191],\n",
       "         ...,\n",
       "         [122, 162, 190],\n",
       "         [110, 153, 180],\n",
       "         [ 91, 134, 161]],\n",
       " \n",
       "        [[108, 171, 185],\n",
       "         [107, 170, 184],\n",
       "         [146, 209, 223],\n",
       "         ...,\n",
       "         [110, 150, 178],\n",
       "         [ 88, 131, 158],\n",
       "         [ 95, 138, 165]],\n",
       " \n",
       "        [[ 99, 162, 176],\n",
       "         [ 88, 151, 165],\n",
       "         [117, 180, 194],\n",
       "         ...,\n",
       "         [ 89, 129, 157],\n",
       "         [ 92, 135, 162],\n",
       "         [ 86, 129, 156]]], dtype=uint8)\n",
       " orig_shape: (2048, 2048)\n",
       " path: '/home/zeloada/Flatiron/Yolo_Detection_of_Bacteria_on_Agar/datasets/data/bacteria_data/images/test/13512.jpg'\n",
       " probs: None\n",
       " speed: {'preprocess': 9.470701217651367, 'inference': 3.3915042877197266, 'postprocess': 1.0581016540527344}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# perform test prediction\n",
    "model.predict(source='./datasets/data/bacteria_data/images/test/13512.jpg', save=True, show=True, imgsz=640, conf=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of second model \n",
    "Fresh start on this model. The image size was doubled and it was trained for 3.33 times as long as the initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second model was constructed and ran for 10 epochs. Image size was doubled\n",
    "second_model = YOLO('./yolov8.yaml')\n",
    "second_model.train(data='./data.yaml', epochs=10, imgsz = 1280, batch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "Model construction has not changed. Yet, through utilizing the pretrained network from second_model, I was able to improve the accuracy drastically. Please see the README for further information. A summary from there is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model was constructed via the pretrain weights of the second_model and ran for max 100 epochs\n",
    "final_model = YOLO('./runs/detect/train6/weights/best.pt') #location of previous model with pre-trained weights\n",
    "final_model.train(data='./data.yaml', patience=10, imgsz = 1280, batch=8) # stopped itself at 98 epochs after no improvement\n",
    "final_model.val() # validation ran on test set\n",
    "final_model.predict(source='./datasets/data/bacteria_data/images/test/3645.jpg', save= True) # save an example image locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment\n",
    "Annotated application that was used. This application was helped in its construction by a member of ZeroEyes, Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, flash, request, redirect, url_for\n",
    "from werkzeug.utils import secure_filename\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from base64 import b64encode\n",
    "import matplotlib\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import webcolors\n",
    "\n",
    "model = YOLO('models_train/97epoch.pt') # instantiate the model\n",
    "UPLOAD_FOLDER = 'upload_folder' # define were files will be saved\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True) # create directory for save if none there\n",
    "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'} # define allowed files\n",
    "app = Flask(__name__) #instatiate flask application\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER # set upload folder\n",
    "def allowed_file(filename): # check if uploaded file has an allowed extension\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "@app.route('/', methods=['GET', 'POST']) #define GET and POST methods\n",
    "def upload_file():\n",
    "    if request.method == 'POST':\n",
    "        if 'file' not in request.files: # if no file is submitted\n",
    "            flash('No file part')\n",
    "            return redirect(request.url)\n",
    "        file = request.files['file']\n",
    "        if file and allowed_file(file.filename): # if there is a file and it's allowed continue\n",
    "            filename = secure_filename(file.filename) # ensure safe file\n",
    "            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))# save file\n",
    "            # img = Image.open(f'{UPLOAD_FOLDER}/{filename}')\n",
    "            img = cv2.imread(f'{UPLOAD_FOLDER}/{filename}') # open file\n",
    "            output_img, classes, colorHex = inference_img(img) # perform inference\n",
    "            _, buffer = cv2.imencode('.jpg', output_img) #encode image from jpg\n",
    "            b64_img = b64encode(buffer).decode() # encode butter file for html display\n",
    "            ## HTML for post\n",
    "            return f''' \n",
    "            <!doctype html>\n",
    "            <h1> Your Agar Plate with Detections </h1>\n",
    "            <img src=data:image/jpeg;base64,{b64_img} width=\"960\" height=\"960\">\n",
    "            <h3> Colonies per class </h3>\n",
    "            <p> {json.dumps(classes)} </p>\n",
    "            <h3> LEGEND </h3>\n",
    "            <p style= \"color:{colorHex['S.aureus']};\">S.aureus</p>\n",
    "            <p style= \"color:{colorHex['B.subtilis']};\">B.subtilis</p>\n",
    "            <p style= \"color:{colorHex['P.aeruginosa']};\">P.aeruginosa</p>\n",
    "            <p style= \"color:{colorHex['E.coli ']};\">E.coli</p>\n",
    "            <p style= \"color:{colorHex['C.albicans']};\">C.albicans</p>\n",
    "            <form method=\"GET\" action=\"/\">  \n",
    "                <input Home type=\"submit\" value=\"Test another image\"/>  \n",
    "            </form>    \n",
    "            </form>\n",
    "            '''\n",
    "    else:\n",
    "        return '''\n",
    "        <!doctype html>\n",
    "        <title>Upload new File</title>\n",
    "        <h1>Upload new File</h1>\n",
    "        <form method=post enctype=multipart/form-data>\n",
    "        <input type=file name=file>\n",
    "        <input type=submit value=Upload>\n",
    "        </form>\n",
    "        '''\n",
    "def inference_img(img: np.array) -> np.array: # take in image as numpy array and return a numpy array\n",
    "    imgsize = 1280\n",
    "    results = model.predict(img, imgsz=imgsize) #define image size\n",
    "    output_img = img.copy() # copy image for bbox drawing\n",
    "    # define classes\n",
    "    class_maps = [\n",
    "        \"S.aureus\",\n",
    "        \"B.subtilis\", \n",
    "        \"P.aeruginosa\",\n",
    "        \"E.coli \",\n",
    "        \"C.albicans\",\n",
    "    ]\n",
    "    colorsRGB = matplotlib.cm.tab20(range(len(class_maps))) # import colors\n",
    "    colors = [(i[:-1][::-1]*255) for i in colorsRGB] # convert colors to portion of 255 in BGR format\n",
    "    colorsRev = [(i[:-1][::1]*255) for i in colorsRGB] # convert colors to portion of 255 in RGB format\n",
    "    colorsTuple = [(int(x),int(y),int(z)) for x,y,z in colorsRev] # convert form list to tuple\n",
    "    colorHex = {x:webcolors.rgb_to_hex(y) for x, y in zip(class_maps, colorsTuple)} # convert RBG to hex code\n",
    "    print(colors)\n",
    "    classes_found = defaultdict(int) # instaiate dictionary with found colonies\n",
    "    for result in results: # take bboxes and draw them on to image\n",
    "        boxes = result.boxes.to('cpu').numpy()\n",
    "        classes = boxes.cls.astype(int)\n",
    "        for box, cls in zip(boxes, classes):\n",
    "            bbox_class = class_maps[cls]\n",
    "            coord = box.xyxy.astype(int).squeeze() # return bbox coord in smallest format\n",
    "            xmin, ymin, xmax, ymax = coord\n",
    "            classes_found[bbox_class] += 1 # count colonies\n",
    "            \n",
    "            color = colors[cls] # define color by class\n",
    "            color = tuple(color) # convert color to a tuple\n",
    "\n",
    "            cv2.rectangle(output_img, (xmin, ymin), (xmax, ymax), color, 2) # draw rectangle on image\n",
    "    print(classes_found)\n",
    "    return output_img, classes_found, colorHex # return image with bboxes, colony count, and hex values for colors used\n",
    "if __name__ == '__main__': # run application\n",
    "    app.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "My final YOLO model takes images as input and consists of __many__ hidden layers. The first layer is a convolutional layer that applies a set of filters to the input image to detect certain features in the image. The output is then passed to the next layer to detect more complicated features. This continues until the final output layer. YOLO utilizes parrellel processing of the image to classify objects as well as regress around the objects. This allows the model to be much faster than dual stage detectors such as faster RCNN. For those of you interested, here is [Ultralytic's github](https://github.com/ultralytics/ultralytics). \n",
    "\n",
    "\n",
    "During training, the model continues to refine the layers and adjust the weight of specific neurons until it can no longer improve itself. After 10 epochs of no improvement, the model will stop itself and save the model at whichever epoch had the highest score. My model precede to run for 87 epochs before leveling out and stopping itself at 97 epochs. This model was then saved and utilized in the application deployment. \n",
    "\n",
    "\n",
    "After training, the test set is introduced to the model to test the score on unseen data. Here are the final scores:\n",
    "\n",
    "Metrics: \n",
    "\n",
    "* __mAP50__ = 0.971\n",
    "* __mAP50-95__ = 0.701\n",
    "\n",
    "This represents a 76% and 129% improvement over the baseline in mAP50 and mAP50-95 respectively.\n",
    "\n",
    "Below is a summary of all results:\n",
    "| box_loss | cls_loss | dfl_loss | precision | recall | mAP50 | map50-95 |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0.969  | 0.39184  | 1.0861  | 0.966  | 0.946  | 0.971  | 0.701  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
